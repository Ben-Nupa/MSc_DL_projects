{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"results\"\n",
    "PATH_TO_DATA = \"data\"\n",
    "training_logs = 'training_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec:\n",
    "    def __init__(self, fname: str, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = {w: i for i, w in enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(list(self.word2vec.values()))\n",
    "\n",
    "    def load_wordvec(self, fname: str, nmax: int):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w: str, K=5) -> list:\n",
    "        \"\"\"Computes the K most similar words of the given word, using class function 'score'.\"\"\"\n",
    "        scores = np.array([self.score(w, w_vocab) if w_vocab != w else -1 for w_vocab in self.word2vec.keys()])\n",
    "        idx_most_similar = np.argsort(-1 * scores)[:K]\n",
    "        words_most_similar = [self.id2word[i] for i in idx_most_similar]\n",
    "        return words_most_similar\n",
    "\n",
    "    def score(self, w1: str, w2: str) -> float:\n",
    "        \"\"\"Computes the cosine similarity of the 2 given words\"\"\"\n",
    "        w1_embedded = self.word2vec[w1]\n",
    "        w2_embedded = self.word2vec[w2]\n",
    "        return np.dot(w1_embedded, w2_embedded) / (np.linalg.norm(w1_embedded) * np.linalg.norm(w2_embedded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "Load duration : 10.818530797958374seconds\n",
      "\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "---------\n",
      "cat ['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "dog ['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "dogs ['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "paris ['france', 'Paris', 'london', 'berlin', 'tokyo']\n",
      "germany ['austria', 'europe', 'german', 'berlin', 'poland']\n",
      "Most similar duration : 4.973707675933838seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "print(\"Load duration : \" + str(time.time() - begin) + \"seconds\\n\")\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "\n",
    "print(\"---------\")\n",
    "\n",
    "begin = time.time()\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w1, w2v.most_similar(w1))\n",
    "print(\"Most similar duration : \" + str(time.time() - begin) + \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV:\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v: Word2vec = w2v\n",
    "    \n",
    "    def sentence_to_list(self, sentence: str) -> list:\n",
    "        \"\"\"Turn string into list by removing spaces and splitting compound words.\"\"\"\n",
    "        return sentence.replace('-', ' ').split()\n",
    "\n",
    "    def encode(self, sentences: list, idf=False) -> np.array:\n",
    "        \"\"\"Takes a list of sentences, outputs a numpy array of sentence embeddings by computing\n",
    "        the (idf-weighted) mean of words vector.\"\"\"\n",
    "        sentences_embedded = []\n",
    "        for sent in sentences:\n",
    "            sent = self.sentence_to_list(sent)\n",
    "            words_weights = []\n",
    "            words_embedded = []\n",
    "            for word in sent:\n",
    "                # Get embedding vector of each word of the sentence\n",
    "                try:\n",
    "                    words_embedded.append(self.w2v.word2vec[word])\n",
    "                    words_weights.append(1 if idf is False else idf[word])  # Get weight of current word (idf or 1)\n",
    "                except KeyError:\n",
    "                    # If word is unknown, ignore it.\n",
    "                    if len(words_embedded) == len(words_weights) + 1:  # 2 different datasets are used\n",
    "                        words_weights.append(0)\n",
    "                    continue\n",
    "            # Average\n",
    "            if len(words_embedded) > 0:\n",
    "                sentences_embedded.append(np.average(words_embedded, weights=words_weights, axis=0))\n",
    "        return np.array(sentences_embedded)\n",
    "\n",
    "    def most_similar(self, s: str, sentences: list, idf=False, K=5) -> list:\n",
    "        \"\"\"Gets most similar sentences and **prints** them.\"\"\"\n",
    "        scores = np.array([self.score(s, sent, idf) if sent != s else -1 for sent in sentences])\n",
    "        idx_most_similar = np.argsort(-1 * scores)[:K]\n",
    "        sentences_most_similar = [sentences[i] for i in idx_most_similar]\n",
    "        print(str(K) + ' most similar sentences with : \"' + s + '\"\\n')\n",
    "        print(sentences_most_similar)\n",
    "        return sentences_most_similar\n",
    "\n",
    "    def score(self, s1: str, s2: str, idf=False):\n",
    "        \"\"\"Computes the cosine similarity of the 2 given sentences\"\"\"\n",
    "        s1_encoded = self.encode([s1], idf).reshape(-1,)\n",
    "        s2_encoded = self.encode([s2], idf).reshape(-1,)\n",
    "        if np.shape(s1_encoded) == (0,) or np.shape(s2_encoded) == (0,):  # Unknown sentence: irrelevant.\n",
    "            return -1\n",
    "        return np.dot(s1_encoded, s2_encoded) / (np.linalg.norm(s1_encoded) * np.linalg.norm(s2_encoded))\n",
    "\n",
    "    def build_idf(self, sentences: list) -> dict:\n",
    "        \"\"\"Builds the idf dictionary: associates each word to its idf value.\"\"\"\n",
    "        idf = {}\n",
    "        # Computes the number of documents where each term appears\n",
    "        for sent in sentences:\n",
    "            sent = self.sentence_to_list(sent)\n",
    "            for i, word in enumerate(sent):\n",
    "                if word not in sent[:i]:  # Consider a term only once per document\n",
    "                    idf[word] = idf.get(word, 0) + 1\n",
    "        # Computes the idf\n",
    "        for word in idf.keys():\n",
    "            idf[word] = np.log10(len(sentences) / idf[word])\n",
    "        return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "5 most similar sentences with : \"1 smiling african american boy .\"\n",
      "\n",
      "['blond boy waterskiing .', 'a boy surfs .', 'a boy jumps .', 'a boy blowing bubbles .', 'a boy wears sunglasses .']\n",
      "-------------\n",
      "\"1 man singing and 1 man playing a saxophone in a concert .\" --- \"11 cheerleaders in yellow and white on a football field , 8 of which are doing a lift .\" --> 0.6089445116147133\n",
      "\n",
      "-------------\n",
      "\n",
      "5 most similar sentences with : \"1 smiling african american boy .\"\n",
      "\n",
      "['a man rides a 4 wheeler in the desert .', 'a man in black is juggling 3 flamed bottles .', '5 women and 1 man are smiling for the camera .', '3 males and 1 woman enjoying a sporting event', 'a young boy in the water in 2 floating tubes .']\n",
      "-------------\n",
      "0.5536982553441099\n"
     ]
    }
   ],
   "source": [
    "def read_sentences(filename: str) -> list:\n",
    "    \"\"\"Extracts the sentences from the given file and returns them in a list.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()\n",
    "        sentences = [line.rstrip() for line in content]  # Remove final newline character\n",
    "        return sentences\n",
    "\n",
    "\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = read_sentences(os.path.join(PATH_TO_DATA, 'sentences.txt'))\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar(sentences[10], sentences)  # BoV-mean\n",
    "print('-------------')\n",
    "print('\"' + sentences[7] + '\" --- \"' + sentences[15] + '\" -->', s2v.score(sentences[7], sentences[13]))\n",
    "print('\\n-------------\\n')\n",
    "\n",
    "s2v.most_similar(sentences[10], sentences, idf)  # BoV-idf\n",
    "print('-------------')\n",
    "print(s2v.score(sentences[7], sentences[13], idf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data :\n",
      "(50000, 300) (50000, 300)\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_voc_data(nb_vectors=50000) -> tuple:\n",
    "    \"\"\"Extracts the first given vectors from the French and English vocabulary data.\"\"\"\n",
    "    def extract_sentences(filename: str) -> tuple:\n",
    "        \"\"\"Extracts the vectors and labels from the vocabulary data.\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            vectors = []\n",
    "            labels = []\n",
    "            for i, line in enumerate(file):\n",
    "                if i > 0:\n",
    "                    splitted_line = line.split()\n",
    "                    # A label can appear several times: we just take it once\n",
    "                    if splitted_line[0] not in labels:\n",
    "                        try:\n",
    "                            values = np.array(splitted_line[1:]).astype(float)\n",
    "                            # Few problems with dimensions, we ignore them\n",
    "                            if values.shape != (300,):\n",
    "                                continue\n",
    "                            else:\n",
    "                                vectors.append(values)\n",
    "                                labels.append(splitted_line[0])\n",
    "                        except:  # Ignore if some weird label appears like 'france »'\n",
    "                            continue\n",
    "                    if len(labels) == nb_vectors:\n",
    "                        return np.array(vectors), labels\n",
    "    \n",
    "    en_vectors, en_labels = extract_sentences(os.path.join(PATH_TO_DATA, 'wiki.en.vec'))\n",
    "    fr_vectors, fr_labels = extract_sentences(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'))\n",
    "    return en_vectors, en_labels, fr_vectors, fr_labels\n",
    "\n",
    "en_vectors, en_labels, fr_vectors, fr_labels = load_voc_data(50000)\n",
    "print('Loaded data :')\n",
    "print(en_vectors.shape, fr_vectors.shape)\n",
    "print(len(en_labels), len(fr_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18982 common words.\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "common_words = []\n",
    "X = []\n",
    "Y = []\n",
    "for i, french_word in enumerate(fr_labels):\n",
    "    for j, english_word in enumerate(en_labels):\n",
    "        if french_word == english_word:\n",
    "            common_words.append(french_word)\n",
    "            X.append(fr_vectors[i])\n",
    "            Y.append(en_vectors[j])\n",
    "            break\n",
    "\n",
    "print('Found ' + str(len(common_words)) + ' common words.')\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from scipy.linalg import svd\n",
    "\n",
    "U, _, Vh = svd(Y.T.dot(X))\n",
    "W = U.dot(Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar translation for \"serbo\" (en_fr) :\n",
      "['serbo', 'bosniaque', 'monténégrin', 'croate', 'serbe']\n",
      "------\n",
      "Most similar translation for \"mollusque\" (fr_en) :\n",
      "['mollusc', 'bivalve', 'bivalves', 'crustacean', 'snail']\n",
      "------\n",
      "Most similar translation for \"tripoli\" (en_fr) :\n",
      "['tripoli', 'beyrouth', 'tripolitaine', 'benghazi', 'tunis']\n",
      "------\n",
      "Most similar translation for \"dénouement\" (fr_en) :\n",
      "['cliffhanger', 'melodramatic', 'unfolds', 'intrigue', 'subplot']\n",
      "------\n",
      "Most similar translation for \"bryn\" (fr_en) :\n",
      "['bryn', 'mawr', 'haverford', 'glyn', 'aberystwyth']\n",
      "------\n",
      "Most similar translation for \"foch\" (en_fr) :\n",
      "['foch', 'joffre', 'weygand', 'gaulle', 'stalingrad']\n",
      "------\n",
      "Most similar translation for \"electricity\" (en_fr) :\n",
      "['électricité', 'hydroélectricité', 'énergie', 'photovoltaïque', 'kwh']\n",
      "------\n",
      "Most similar translation for \"établissement\" (fr_en) :\n",
      "['conseil', 'establishment', 'institution', 'collège', 'institutions']\n",
      "------\n",
      "Most similar translation for \"jailed\" (en_fr) :\n",
      "['incarcéré', 'emprisonné', 'inculpé', 'incarcérés', 'condamné']\n",
      "------\n",
      "Most similar translation for \"fourier\" (fr_en) :\n",
      "['fourier', 'cauchy', 'laplace', 'convolution', 'legendre']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "import random\n",
    "\n",
    "\n",
    "def most_similar(word, direction='fr_en', K=5) -> list:\n",
    "    \"\"\"Computes the K most similar words of the given word, using function 'score'.\"\"\"\n",
    "    # Check direction of translation\n",
    "    if direction == 'fr_en':\n",
    "        X_mat = fr_vectors\n",
    "        Y_mat = en_vectors\n",
    "        W_mat = W\n",
    "        source_labels = fr_labels\n",
    "        target_labels = en_labels\n",
    "    elif direction == 'en_fr':\n",
    "        X_mat = en_vectors\n",
    "        Y_mat = fr_vectors\n",
    "        W_mat = W.T\n",
    "        source_labels = en_labels\n",
    "        target_labels = fr_labels\n",
    "    \n",
    "    # Get the source word\n",
    "    if type(word) == str:\n",
    "        idx_word = source_labels.find(word)\n",
    "    else:\n",
    "        idx_word = word\n",
    "    \n",
    "    # Compute scores and nearest neighbors\n",
    "    scores = np.array([cosine_similarity(W_mat.dot(X_mat[idx_word]), y) for y in Y_mat])\n",
    "    idx_most_similar = np.argsort(-1 * scores)[:K]\n",
    "    words_most_similar = [target_labels[i] for i in idx_most_similar]\n",
    "    print('Most similar translation for \"' + source_labels[idx_word] + '\" (' + direction + ') :')\n",
    "    print(words_most_similar)\n",
    "    return words_most_similar\n",
    "\n",
    "def cosine_similarity(v1: np.array, v2: np.array) -> float:\n",
    "    \"\"\"Computes the cosine similarity of the 2 given vectors\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Try a few words:\n",
    "for i in range(10):\n",
    "    idx_word = random.randint(0, len(fr_labels))\n",
    "    direction = random.choice(['fr_en', 'en_fr'])\n",
    "    most_similar(idx_word, direction)\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_sst_data(folder: str) -> tuple:\n",
    "    \"\"\"Extracts the SST data from the given folder (sentences and labels from each set).\"\"\"\n",
    "    def extract_sentences(filename: str, is_testing_set=False) -> tuple:\n",
    "        \"\"\"Extracts the sentences and labels from the given file from the SST dataset.\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            sentences = []\n",
    "            labels = []\n",
    "            for line in file.readlines():\n",
    "                if is_testing_set:  # No label in testing set\n",
    "                    sentences.append(line)\n",
    "                else:\n",
    "                    labels.append(int(line[0]))\n",
    "                    sentences.append(line[2:])\n",
    "        return sentences, labels\n",
    "\n",
    "    tr_sentences, tr_labels = extract_sentences(os.path.join(folder, 'stsa.fine.train'))\n",
    "    val_sentences, val_labels = extract_sentences(os.path.join(folder, 'stsa.fine.dev'))\n",
    "    te_sentences, _ = extract_sentences(os.path.join(folder, 'stsa.fine.test.X'), is_testing_set=True)\n",
    "\n",
    "    return tr_sentences, tr_labels, val_sentences, val_labels, te_sentences\n",
    "\n",
    "\n",
    "tr_sentences, tr_labels, val_sentences, val_labels, te_sentences = load_sst_data(os.path.join(PATH_TO_DATA, 'SST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=50000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Decide whether to use IDF or not\n",
    "idf = False\n",
    "# idf = s2v.build_idf(tr_sentences)  # Build IDF only on the testing set\n",
    "\n",
    "X_tr = s2v.encode(tr_sentences, idf)\n",
    "X_val = s2v.encode(val_sentences, idf)\n",
    "X_te = s2v.encode(te_sentences, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training set =  0.4853698501872659\n",
      "Accuracy validation set =  0.4332425068119891\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create PredefinedSplit object for fixed cross validation\n",
    "X_all = np.vstack((X_tr, X_val))\n",
    "y_all = tr_labels + val_labels\n",
    "seperation_indices = [-1 for i in range(len(tr_labels))]  # Training part\n",
    "seperation_indices += [1 for i in range(len(val_labels))]  # Validation part\n",
    "ps = PredefinedSplit(seperation_indices)\n",
    "\n",
    "\"\"\"\n",
    "Fine-tune 'C' value\n",
    "Uncomment the following part to test it\n",
    "\"\"\"\n",
    "# parameters = {'C': np.logspace(-4, 4, 20)}\n",
    "# clf = GridSearchCV(LogisticRegression(), param_grid=parameters, cv=ps)\n",
    "# clf.fit(X_all, y_all)\n",
    "# print(clf.best_params_)\n",
    "\n",
    "# Test with best C = 1.623776739188721\n",
    "clf = LogisticRegression(C=1.623776739188721)\n",
    "clf.fit(X_tr, tr_labels)\n",
    "\n",
    "print('Accuracy training set = ', accuracy_score(tr_labels, clf.predict(X_tr)))\n",
    "print('Accuracy validation set = ', accuracy_score(val_labels, clf.predict(X_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "y_predicted = clf.predict(X_te)\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "with open(os.path.join(output_folder, 'logreg_bov_y_test_sst.txt'), 'w') as file:\n",
    "    for i in y_predicted:\n",
    "        file.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training set =  0.4785814606741573\n",
      "Accuracy validation set =  0.4232515894641235\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# PCA\n",
    "pca = PCA(200)\n",
    "X_tr_reduced = pca.fit_transform(X_tr)\n",
    "X_val_reduced = pca.transform(X_val)\n",
    "X_te_reduced = pca.transform(X_te)\n",
    "\n",
    "\"\"\"\n",
    "Fine-tune 'C' value.\n",
    "Uncomment the following part to test it\n",
    "\"\"\"\n",
    "# Create PredefinedSplit object for fixed cross validation\n",
    "# X_all_reduced = np.vstack((X_tr_reduced, X_val_reduced))\n",
    "# y_all = tr_labels + val_labels\n",
    "# seperation_indices = [-1 for i in range(len(tr_labels))]  # Training part\n",
    "# seperation_indices += [1 for i in range(len(val_labels))]  # Validation part\n",
    "# ps = PredefinedSplit(seperation_indices)\n",
    "# parameters = {'C': np.logspace(-4, 4, 20)}\n",
    "# better_clf = GridSearchCV(SVC(), param_grid=parameters, cv=ps, verbose=True)\n",
    "# better_clf.fit(X_all_reduced, y_all)\n",
    "# print(better_clf.best_params_)\n",
    "\n",
    "# Test with best C = 206.913808111479\n",
    "better_clf = SVC(C=206.913808111479)\n",
    "better_clf.fit(X_tr_reduced, tr_labels)\n",
    "\n",
    "print('Accuracy training set = ', accuracy_score(tr_labels, better_clf.predict(X_tr_reduced)))\n",
    "print('Accuracy validation set = ', accuracy_score(val_labels, better_clf.predict(X_val_reduced)))\n",
    "\n",
    "# Save\n",
    "y_predicted = better_clf.predict(X_te_reduced)\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "with open(os.path.join(output_folder, 'svm_bov_y_test_sst.txt'), 'w') as file:\n",
    "    for i in y_predicted:\n",
    "        file.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_sst_data(folder: str) -> tuple:\n",
    "    \"\"\"Extracts the SST data from the given folder (sentences and labels from each set).\"\"\"\n",
    "    def extract_sentences(filename: str, is_testing_set=False) -> tuple:\n",
    "        \"\"\"Extracts the sentences and labels from the given file from the SST dataset.\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            sentences = []\n",
    "            labels = []\n",
    "            for line in file.readlines():\n",
    "                if is_testing_set:  # No label in testing set\n",
    "                    sentences.append(line)\n",
    "                else:\n",
    "                    labels.append(int(line[0]))\n",
    "                    sentences.append(line[2:])\n",
    "        return sentences, labels\n",
    "\n",
    "    tr_sentences, tr_labels = extract_sentences(os.path.join(folder, 'stsa.fine.train'))\n",
    "    val_sentences, val_labels = extract_sentences(os.path.join(folder, 'stsa.fine.dev'))\n",
    "    te_sentences, _ = extract_sentences(os.path.join(folder, 'stsa.fine.test.X'), is_testing_set=True)\n",
    "\n",
    "    return tr_sentences, tr_labels, val_sentences, val_labels, te_sentences\n",
    "\n",
    "\n",
    "# It's the same function as defined in part 3\n",
    "tr_sentences, y_tr, val_sentences, y_val, te_sentences = load_sst_data(os.path.join(PATH_TO_DATA, 'SST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min sentences length =  1\n",
      "Mean sentences length =  17.049936735554617\n",
      "Median sentences length =  16.0\n",
      "Max sentences length =  52\n",
      "Number of unique words =  19375\n"
     ]
    }
   ],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "import re\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "\n",
    "def get_some_stats(*sentences: list):\n",
    "    \"\"\"Computes and displays the min, max, mean, median sentences length and the number of unique words.\"\"\"\n",
    "    unique_words = set()\n",
    "    sentences_len = []\n",
    "    for sentences_list in sentences:\n",
    "        for sent in sentences_list:\n",
    "            sentences_len.append(len(re.sub(r'[^\\w\\s]', '', sent).split()))\n",
    "            for word in re.sub(r'[^\\w\\s]', '', sent).split():\n",
    "                if word not in unique_words:\n",
    "                    unique_words.add(word)                \n",
    "    print('Min sentences length = ', np.min(sentences_len))\n",
    "    print('Mean sentences length = ', np.mean(sentences_len))\n",
    "    print('Median sentences length = ', np.median(sentences_len))\n",
    "    print('Max sentences length = ', np.max(sentences_len))\n",
    "    print('Number of unique words = ', len(unique_words))\n",
    "\n",
    "\n",
    "get_some_stats(tr_sentences, val_sentences, te_sentences)\n",
    "\n",
    "nb_unique_words = 15000\n",
    "X_tr = [one_hot(line, nb_unique_words, filters='#*+-/<=>@[\\]^_{|}~', lower=False) for line in tr_sentences]\n",
    "X_val = [one_hot(line, nb_unique_words, filters='#*+-/<=>@[\\]^_{|}~', lower=False) for line in val_sentences]\n",
    "X_te = [one_hot(line, nb_unique_words, filters='#*+-/<=>@[\\]^_{|}~', lower=False) for line in te_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8544, 40)\n",
      "(1101, 40)\n",
      "(2210, 40)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# The maximum length of all sequences is = 52 (from the testing set).\n",
    "maxlen = 40\n",
    "X_tr = pad_sequences(X_tr, maxlen=maxlen, padding='post')\n",
    "X_val = pad_sequences(X_val, maxlen=maxlen, padding='post')\n",
    "X_te = pad_sequences(X_te, maxlen=maxlen, padding='post')\n",
    "\n",
    "print(np.shape(X_tr))\n",
    "print(np.shape(X_val))\n",
    "print(np.shape(X_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\applications\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.5, recurrent_dropout=0.5)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, Flatten, Dropout, BatchNormalization, GaussianNoise\n",
    "\n",
    "embed_dim = 128  # word embedding dimension\n",
    "nhid = 64  # number of hidden units in the LSTM\n",
    "vocab_size = nb_unique_words  # size of the vocabulary\n",
    "n_classes = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GaussianNoise(0.01, input_shape=(maxlen,)))  # To diminish the overfitting\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.5, dropout_U=0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_1 (GaussianNo (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 40, 128)           1920000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,969,733\n",
      "Trainable params: 1,969,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "loss_classif = 'sparse_categorical_crossentropy'  # find the right loss for multi-class classification\n",
    "# optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)  # find the right optimizer\n",
    "optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "metrics_classif = ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/12\n",
      "8544/8544 [==============================] - 8s 978us/step - loss: 1.5757 - acc: 0.2617 - val_loss: 1.5779 - val_acc: 0.2625\n",
      "Epoch 2/12\n",
      "8544/8544 [==============================] - 6s 747us/step - loss: 1.5697 - acc: 0.2693 - val_loss: 1.5784 - val_acc: 0.2534\n",
      "Epoch 3/12\n",
      "8544/8544 [==============================] - 6s 727us/step - loss: 1.5660 - acc: 0.2753 - val_loss: 1.5739 - val_acc: 0.2561\n",
      "Epoch 4/12\n",
      "8544/8544 [==============================] - 7s 786us/step - loss: 1.5531 - acc: 0.2808 - val_loss: 1.5764 - val_acc: 0.2816\n",
      "Epoch 5/12\n",
      "8544/8544 [==============================] - 6s 738us/step - loss: 1.4833 - acc: 0.3548 - val_loss: 1.5024 - val_acc: 0.3379\n",
      "Epoch 6/12\n",
      "8544/8544 [==============================] - 7s 837us/step - loss: 1.3776 - acc: 0.4044 - val_loss: 1.5120 - val_acc: 0.3424\n",
      "Epoch 7/12\n",
      "8544/8544 [==============================] - 7s 767us/step - loss: 1.3022 - acc: 0.4316 - val_loss: 1.5275 - val_acc: 0.3560\n",
      "Epoch 8/12\n",
      "8544/8544 [==============================] - 6s 720us/step - loss: 1.2480 - acc: 0.4669 - val_loss: 1.5521 - val_acc: 0.3351\n",
      "Epoch 9/12\n",
      "8544/8544 [==============================] - 6s 745us/step - loss: 1.2001 - acc: 0.4842 - val_loss: 1.5901 - val_acc: 0.3424\n",
      "Epoch 10/12\n",
      "8544/8544 [==============================] - 6s 746us/step - loss: 1.1467 - acc: 0.5153 - val_loss: 1.6532 - val_acc: 0.3433\n",
      "Epoch 11/12\n",
      "8544/8544 [==============================] - 6s 720us/step - loss: 1.1230 - acc: 0.5290 - val_loss: 1.6393 - val_acc: 0.3551\n",
      "Epoch 12/12\n",
      "8544/8544 [==============================] - 6s 719us/step - loss: 1.0837 - acc: 0.5423 - val_loss: 1.6461 - val_acc: 0.3642\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAGphJREFUeJzt3X903XWd5/Hnq2liKUUnpHGP219JnY5QWgLtbYirsGz5YVml5ajn0MrMqTJYWenCDnp2QHBEOMfD4uqox85irXU4ZyuZGRnHwFGQ8svOWWObIMi0gJTQ2lDUkmZRlv5K894/7m29pKG5Se6939z7fT3O6bn3+/1+vve+v0n6yjfv+/2hiMDMzNJhUtIFmJlZ+Tj0zcxSxKFvZpYiDn0zsxRx6JuZpYhD38wsRRz6ZmYp4tA3M0sRh76ZWYpMTrqAoaZPnx5NTU1Jl2FmVlG6u7tfjYjGkcYVFPqSlgFfB2qADRFx55DlHwe+DLycm/XNiNiQW3YUeCY3/9cRsfxk79XU1ERXV1chZZmZWY6k3YWMGzH0JdUA64BLgF5gm6SOiNgxZOg/RMTaYV7iQEScU0gxZmZWWoX09FuBnRHRExGHgXZgRWnLMjOzUigk9GcAe/Kme3PzhvqIpF9K+r6kWXnzp0jqktQp6YrxFGtmZuNTSE9fw8wbej3m+4F7I+KQpGuBe4CluWWzI2KvpLnAo5KeiYgX3/QG0hpgDcDs2bNPeLMjR47Q29vLwYMHCyg3faZMmcLMmTOpra1NuhQzm+AKCf1eIH/PfSawN39ARPTlTX4b+B95y/bmHnskPQ6cC7w4ZP31wHqATCZzwgX+e3t7Oe2002hqakIa7ndQekUEfX199Pb20tzcnHQ5ZjbBFdLe2QbMk9QsqQ5YCXTkD5D0rrzJ5cCzufn1kt6Wez4deB8w9APgER08eJCGhgYH/jAk0dDQ4L+CzKwgI+7pR8SApLXAQ2QP2dwYEdsl3Q50RUQHcL2k5cAAsB/4eG71M4FvSRok+wvmzmGO+imIA/+t+WtjVvm6d/fT2dNH29wGFs+pL9n7FHScfkT8CPjRkHl/k/f8ZuDmYdb7P8DCcdZoZlbVunf3c9WGTg4PDFI3eRKbrmkrWfBPuDNyJ6K+vj4uuugiAH7zm99QU1NDY2P2xLetW7dSV1eXZHlmVuE6e/o4PDDIYMCRgUE6e/oc+klqaGjgqaeeAuC2225j2rRpfPazn33TmIggIpg0yZczMrPRaZvbQN3kSRwZGKR28iTa5jaU7L2qNqG6d/ez7rGddO/uL9l77Ny5kwULFnDttdeyaNEiXnnllTct/8IXvsCSJUuOj4nIHpj0q1/9iqVLl9LS0sKiRYvYtWsXAF/60pdYuHAhLS0t3HLLLSWr28wmlsVz6tl0TRs3XvqekrZ2oEr39MvZH9uxYwff/e53ufvuu09YdsMNN/DFL36RiOBjH/sYDz74IJdddhmrVq3itttu4/LLL+fgwYMMDg5y//338+Mf/5itW7dyyimnsH///pLUa2YT0+I59SUN+2Oqck9/uP5Yqbz73e9myZIlwy575JFHaG1tpaWlhSeeeILt27fT39/Pq6++yuWXXw5kT6yaOnUqmzdv5uqrr+aUU04B4PTTTy9ZzWaWXlW5p1/O/tipp5467Pw33niDtWvX8uSTTzJjxgxuvfXW48fSD3eIZUT40EszK7mq3NMvZ3/srRw4cIBJkyYxffp0/vCHP3DfffcBUF9fz/Tp07n//vuB7Ilnb7zxBpdeeinf+c53OHDgAIDbO2ZWElW5pw/l64+9lYaGBlavXs2CBQuYM2cO55133vFlmzZt4lOf+hS33HILdXV13HfffXzoQx/i6aefJpPJUFtby+WXX84dd9yRWP1maVeuk6XKTceOKJkoMplMDL2JyrPPPsuZZ56ZUEWVwV8js+Ip58EgxSKpOyIyI42ryvaOmdl4lPNgkHJz6JuZDXHsYJAaUfKDQcqtanv6ZmZjdexgkGrs6Tv0zcyGkfTBIKXi9o6ZWYo49M3MUsShX4ALL7yQhx566E3zvva1r/HpT3/6pOtNmzatlGWZmY2aQ78Aq1ator29/U3z2tvbWbVqVUIVmZmNTfWG/p6tsOUr2cdx+uhHP8oDDzzAoUOHANi1axd79+7l/e9/P6+//joXXXQRixYtYuHChfzwhz8c8fWuuOIKFi9ezFlnncX69euPz3/wwQdZtGgRLS0tx2/a8vrrr/OJT3yChQsXcvbZZx+/nINZGpXjkunVrjqP3tmzFe5ZDkcPQ00drO6AWa1jfrmGhgZaW1t58MEHWbFiBe3t7Vx55ZVIYsqUKfzgBz/g7W9/O6+++iptbW0sX778pBdP27hxI6effjoHDhxgyZIlfOQjH2FwcJBPfvKT/PSnP6W5ufn4tXfuuOMO3vGOd/DMM88A0N/vH3ZLp0o8S3Yiqs49/V1bsoEfR7OPu7aM+yXzWzz5rZ2I4HOf+xxnn302F198MS+//DK//e1vT/pa3/jGN2hpaaGtrY09e/bwwgsv0NnZyQUXXEBzczPwx0srb968meuuu+74uvX1/iG3dKrms2TLqTr39JvOz+7hH9vTbzp/3C95xRVXcOONN/Lkk09y4MABFi1aBGQvnrZv3z66u7upra2lqanp+CWUh/P444+zefNmfvaznzF16lQuvPBCDh48+JaXVvYll82yynnJ9GpWnXv6s1qzLZ2lt4y7tXPMtGnTuPDCC7n66qvf9AHua6+9xjvf+U5qa2t57LHH2L1790lf57XXXqO+vp6pU6fy3HPP0dnZCcB73/tennjiCV566SXgj5dWvvTSS/nmN795fH23dyytJsIl06tBdYY+ZIP+/M8UJfCPWbVqFU8//TQrV648Pu+qq66iq6uLTCbDpk2bOOOMM076GsuWLWNgYICzzz6bz3/+87S1tQHQ2NjI+vXr+fCHP0xLSwtXXnklALfeeiv9/f0sWLCAlpYWHnvssaJtj1mlWTynnuv+05868MfBl1auEv4amaWbL61sZmYnKCj0JS2T9LyknZJuGmb5xyXtk/RU7t81ectWS3oh9291MYs3M7PRGfHoHUk1wDrgEqAX2CapIyJ2DBn6DxGxdsi6pwNfADJAAN25dUf9aaSPYnlrE61FZ2YTVyF7+q3AzojoiYjDQDuwosDX/wDwcETszwX9w8Cy0RY5ZcoU+vr6HG7DiAj6+vqYMmVK0qWYWQUo5Dj9GcCevOle4Lxhxn1E0gXAr4C/iog9b7HujNEWOXPmTHp7e9m3b99oV02FKVOmMHPmzKTLMLMKUEjoD9dTGbrLfT9wb0QcknQtcA+wtMB1kbQGWAMwe/bsE1aora09fqaqmZmNXSHtnV5gVt70TGBv/oCI6IuIQ7nJbwOLC103t/76iMhERKaxsbHQ2s3MbJQKCf1twDxJzZLqgJVAR/4ASe/Km1wOPJt7/hBwqaR6SfXApbl5ZmaWgBHbOxExIGkt2bCuATZGxHZJtwNdEdEBXC9pOTAA7Ac+nlt3v6Q7yP7iALg9IvaXYDvMzKwAFXFGrpmZnZzPyDUzsxM49M3MUsShb2aWIg59M7MUceib2Zj5RuWVpzpvl2hmJecblVcm7+mb2Zj4RuWVyaFvZmNy7EblNcI3Kq8gbu+Y2Zgcu1F5Z08fbXMb3NqpEA59MxuzxXPqHfYVxu0dM7MUceibmaWIQ9/MLEUc+mZmKeLQNzNLEYe+mVmKOPTNqoivhWMj8XH6ZlXC18KxQnhP36xK+Fo4VgiHvlmV8LVwrBBu75hVCV8Lxwrh0DerIr4Wjo3E7R0zsxRx6JuZpYhD38wsRRz6ZmYpUlDoS1om6XlJOyXddJJxH5UUkjK56SZJByQ9lft3d7EKNzOz0Rvx6B1JNcA64BKgF9gmqSMidgwZdxpwPfDzIS/xYkScU6R6zcxsHArZ028FdkZET0QcBtqBFcOMuwO4CzhYxPrMzKyICgn9GcCevOne3LzjJJ0LzIqIB4ZZv1nSLyQ9Ien84d5A0hpJXZK69u3bV2jtZmY2SoWEvoaZF8cXSpOAvwU+M8y4V4DZEXEucCPwPUlvP+HFItZHRCYiMo2NjYVVbmZmo1ZI6PcCs/KmZwJ786ZPAxYAj0vaBbQBHZIyEXEoIvoAIqIbeBH4s2IUbmZmo1dI6G8D5klqllQHrAQ6ji2MiNciYnpENEVEE9AJLI+ILkmNuQ+CkTQXmAf0FH0rzMysICMevRMRA5LWAg8BNcDGiNgu6XagKyI6TrL6BcDtkgaAo8C1EbG/GIWbmdnoKSJGHlVGmUwmurq6ki7DzKyiSOqOiMxI43xGrplZijj0zcxSxKFvZpYiDn0zsxRx6JuZpYhD36zEunf3s+6xnXTv7k+6FDPfI9eslLp393PVhk4ODwxSN3kSm65p8z1sLVHe0zcroc6ePg4PDDIYcGRgkM6evqRLspRz6JuVUNvcBuomT6JGUDt5Em1zG5IuyVLO7R2zElo8p55N17TR2dNH29wGt3YscQ59sxJbPKfeYW8Thts7ZmYp4tA3M0sRh76ZWYo49M3MUsShb2aWIg59M7MUceibmaWIQ9/MLEUc+mZmKeLQNzNLEYe+mVmKOPTNzFLEoW9mliIOfUsd377Q0qyg0Je0TNLzknZKuukk4z4qKSRl8ubdnFvveUkfKEbRZmN17PaFX/nJ81y1odPBb6kzYuhLqgHWAZcB84FVkuYPM+404Hrg53nz5gMrgbOAZcDf5V7PLBG+faGlXSF7+q3AzojoiYjDQDuwYphxdwB3AQfz5q0A2iPiUES8BOzMvZ5ZInz7Qku7Qu6cNQPYkzfdC5yXP0DSucCsiHhA0meHrNs5ZN0ZY6zVbNx8+0JLu0JCX8PMi+MLpUnA3wIfH+26ea+xBlgDMHv27AJKMhs7377Q0qyQ9k4vMCtveiawN2/6NGAB8LikXUAb0JH7MHekdQGIiPURkYmITGNj4+i2wMzMClZI6G8D5klqllRH9oPZjmMLI+K1iJgeEU0R0US2nbM8Irpy41ZKepukZmAesLXoW2FmZgUZsb0TEQOS1gIPATXAxojYLul2oCsiOk6y7nZJ/wjsAAaA6yLiaJFqNzOzUVLECS32RGUymejq6kq6DDOziiKpOyIyI43zGblmZini0DczSxGHvplZijj0zcxSxKFvZpYiDn0zsxRx6JuZpYhD38wsRRz6ZmYp4tA3M0sRh76ZWYo49M3MUsShb2aWIg59M7MUceibmaWIQ9/MLEUc+mZmKeLQNzObCPZshS1fyT6W0Ij3yDUzsxLbsxXuWQ5HD0NNHazugFmtJXkr7+mbmSVt15Zs4MfR7OOuLSV7K4e+mVnSms7P7uGrJvvYdH7J3srtHTOzpM1qzbZ0dm3JBn6JWjvg0DczmxhmtZY07I9xe8es1Mp0VIYVWZV+37ynb4nr3t1PZ08fbXMbWDynPulyiquMR2VYEVXx962gPX1JyyQ9L2mnpJuGWX6tpGckPSXpXyXNz81vknQgN/8pSXcXewOssnXv7ueqDZ185SfPc9WGTrp39yddUnGV8agMK6Iq/r6NGPqSaoB1wGXAfGDVsVDP872IWBgR5wB3AV/NW/ZiRJyT+3dtsQq36tDZ08fhgUEGA44MDNLZ05d0ScVVxqMyrIiq+PtWSHunFdgZET0AktqBFcCOYwMi4vd5408FophFWvVqm9tA3eRJHBkYpHbyJNrmNiRdUnGV8agMK6Iq/r4VEvozgD15073AeUMHSboOuBGoA5bmLWqW9Avg98CtEVE9fyfZuC2eU8+ma9qqt6cPZTsqw4qsSr9vhYS+hpl3wp58RKwD1kn6GHArsBp4BZgdEX2SFgP/IumsIX8ZIGkNsAZg9uzZo9wEq3SL59RXZ9ibTUCFfJDbC8zKm54J7D3J+HbgCoCIOBQRfbnn3cCLwJ8NXSEi1kdEJiIyjY2NhdZuZmajVEjobwPmSWqWVAesBDryB0ialzf5QeCF3PzG3AfBSJoLzAN6ilG4maVQlR47X04jtnciYkDSWuAhoAbYGBHbJd0OdEVEB7BW0sXAEaCfbGsH4ALgdkkDwFHg2ojYX4oNMSvYnq1V+QFdIsr5taziY+fLqaCTsyLiR8CPhsz7m7znN7zFevcB942nQLOicnAUT7m/lsMdO+/v3aj5MgyWLlV80k3ZlftrWcXHzpeTL8NgyStni+BYcBzbO3VwjF25v5ZVfOx8OSliYp1HlclkoqurK+kyrFySaLe4p188/lpOGJK6IyIz0jjv6VuykujTVulJN0D5Q7iav5ZVyqFvyXK7pXj8IbUVwKFvyXKftnh8dIsVwKFvyXOLoDj8V5MVwKFvVi38V5MVwKFvVk38V5ONwCdnmZmliEPfzCxFHPpmZini0DczSxGHvplZijj0zcxSxKFvZpYiDn0zsxRx6JuZpYhD38wsRRz6ZmYp4tA3M0sRh76ZWYo49M3MUsShb2aWIg59M7MUKSj0JS2T9LyknZJuGmb5tZKekfSUpH+VND9v2c259Z6X9IFiFm9mZqMzYuhLqgHWAZcB84FV+aGe872IWBgR5wB3AV/NrTsfWAmcBSwD/i73emZmloBC9vRbgZ0R0RMRh4F2YEX+gIj4fd7kqUDknq8A2iPiUES8BOzMvZ6ZmSWgkHvkzgD25E33AucNHSTpOuBGoA5Ymrdu55B1Z4ypUjMzG7dC9vQ1zLw4YUbEuoh4N/DXwK2jWVfSGkldkrr27dtXQElmZjYWhYR+LzArb3omsPck49uBK0azbkSsj4hMRGQaGxsLKMnMzMaikNDfBsyT1CypjuwHsx35AyTNy5v8IPBC7nkHsFLS2yQ1A/OAreMv28zMxmLEnn5EDEhaCzwE1AAbI2K7pNuBrojoANZKuhg4AvQDq3Prbpf0j8AOYAC4LiKOlmhbzMxsBIo4ocWeqEwmE11dXUmXYWZWUSR1R0RmpHE+I9fMLEUc+mZmKeLQNzNLEYe+mVmKOPTNzFLEoW9mliKFXHvHUui5bZvp3/Eo9fOXcsaSi5Mux8yKxKFvJ3hu22bmPLCKP2WAIz3f5jnudfCbVQm3d+wE/TsepZYBJmuQWgbo3/Fo0iWZWZE49O0E9fOXcoTJDMQkjjCZ+vlLR17JzCqC2zsVont3P509fbTNbWDxnPqSvtcZSy7mOe51T9+sCjn0K0D37n6u2tDJ4YFB6iZPYtM1bWUJfhz2ZlXH7Z0K0NnTx+GBQQYDjgwM0tnTl3RJZlahHPoVoG1uA3WTJ1EjqJ08iba5DUmXZGYVyu2dMSpnj33xnHo2XdNWtvczs+rl0B+DJHrsi+fUO+zNbNzc3hkD99jNrFI59MfAPXYzq1Ru74yBe+xmVqmqKvTLeZEw99jNrBJVTej7ImFmZiOrmp6+LxJmZjayqgl9XyTMzGxkVdPeqfqLhO3ZCru2QNP5MKs16WrMrEJVTehDFV8kbM9WuGc5HD0MNXWwusPBb2ZjUjXtnbLbsxW2fCX7WGq7tmQDP45mH3dtKf17mllVKmhPX9Iy4OtADbAhIu4csvxG4BpgANgHXB0Ru3PLjgLP5Ib+OiKWF6n25JR7z7vp/Oz7HHu/pvNL915mVtVGDH1JNcA64BKgF9gmqSMiduQN+wWQiYg3JP0X4C7gytyyAxFxTpHrTtZwe96lDP1ZrdlfLO7pm9k4FbKn3wrsjIgeAEntwArgeOhHxGN54zuBPy9mkRNOEnves1od9mY2boWE/gxgT950L3DeScb/JfDjvOkpkrrItn7ujIh/GbqCpDXAGoDZs2cXUFLCvOdtZhWqkNDXMPNi2IHSnwMZ4D/mzZ4dEXslzQUelfRMRLz4pheLWA+sB8hkMsO+9oTjPW8zq0CFHL3TC8zKm54J7B06SNLFwC3A8og4dGx+ROzNPfYAjwPnjqNeMzMbh0JCfxswT1KzpDpgJdCRP0DSucC3yAb+7/Lm10t6W+75dOB95H0WYGZm5TVieyciBiStBR4ie8jmxojYLul2oCsiOoAvA9OAf5IEfzw080zgW5IGyf6CuXPIUT9mZlZGiphYLfRMJhNdXV1Jl2FmVlEkdUdEZqRxPiPXzCxFHPpmZiky4do7kvYBu5Ouo0DTgVeTLqKEqnn7vG2Vq5q3bzzbNiciGkcaNOFCv5JI6iqkh1apqnn7vG2Vq5q3rxzb5vaOmVmKOPTNzFLEoT8+65MuoMSqefu8bZWrmrev5Nvmnr6ZWYp4T9/MLEUc+mMgaZakxyQ9K2m7pBuSrqnYJNVI+oWkB5Kupdgk/Ymk70t6Lvc9fG/SNRWLpL/K/Uz+m6R7JU1JuqbxkLRR0u8k/VvevNMlPSzphdxjfZI1jtVbbNuXcz+Xv5T0A0l/Uuz3deiPzQDwmYg4E2gDrpM0P+Gaiu0G4NmkiyiRrwMPRsQZQAtVsp2SZgDXk72L3QKy18pamWxV4/b3wLIh824CHomIecAjuelK9PecuG0PAwsi4mzgV8DNxX5Th/4YRMQrEfFk7vkfyIbGjGSrKh5JM4EPAhuSrqXYJL0duAD4DkBEHI6I/5tsVUU1GThF0mRgKsNcBr2SRMRPgf1DZq8A7sk9vwe4oqxFFclw2xYRP4mIgdxkJ9lL2ReVQ3+cJDWRvUfAz5OtpKi+Bvx3YDDpQkpgLrAP+G6ufbVB0qlJF1UMEfEy8D+BXwOvAK9FxE+Sraok/l1EvALZHTDgnQnXUypX8+a7EBaFQ38cJE0D7gP+W0T8Pul6ikHSh4DfRUR30rWUyGRgEfC/IuJc4P9Rue2BN8n1tlcAzcC/B07N3c3OKoykW8i2kTcV+7Ud+mMkqZZs4G+KiH9Oup4ieh+wXNIuoB1YKul/J1tSUfUCvRFx7C+z75P9JVANLgZeioh9EXEE+GfgPyRcUyn8VtK7AHKPvxthfEWRtBr4EHBVlOCYeof+GCh7p5jvAM9GxFeTrqeYIuLmiJgZEU1kPwR8NCKqZm8xIn4D7JH0ntysi6ieu7n9GmiTNDX3M3oRVfIh9RAdwOrc89XADxOspagkLQP+muxdCN8oxXs49MfmfcBfkN0Lfir37z8nXZQV7L8CmyT9EjgH+FLC9RRF7q+X7wNPAs+Q/f9d0WevSroX+BnwHkm9kv4SuBO4RNILwCW56YrzFtv2TeA04OFcrtxd9Pf1GblmZunhPX0zsxRx6JuZpYhD38wsRRz6ZmYp4tA3M0sRh76ZWYo49M3MUsShb2aWIv8fqnuKPBhvi0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Tensorboard to follow training\n",
    "if not os.path.exists(training_logs):\n",
    "    os.makedirs(training_logs)\n",
    "tensorboard = TensorBoard(log_dir=training_logs)\n",
    "\n",
    "# Fit\n",
    "bs = 128\n",
    "n_epochs = 12\n",
    "history = model.fit(X_tr, y_tr, batch_size=bs, epochs=n_epochs, validation_data=(X_val, y_val), callbacks=[tensorboard])\n",
    "\n",
    "# Plot training\n",
    "abscissa = 1 + np.arange(n_epochs)\n",
    "# Loss\n",
    "plt.figure()\n",
    "plt.plot(abscissa, history.history['loss'], '.', label='Tr loss')\n",
    "plt.plot(abscissa, history.history['val_loss'], '.', label='Val loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.figure()\n",
    "plt.plot(abscissa, history.history['acc'], '.', label='Tr acc')\n",
    "plt.plot(abscissa, history.history['val_acc'], '.', label='Val acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "y_predicted = model.predict(X_te).argmax(axis=-1)\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "with open(os.path.join(output_folder, 'logreg_lstm_y_test_sst.txt'), 'w') as file:\n",
    "    for i in y_predicted:\n",
    "        file.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>/!\\ Read this cell</font>**\n",
    "\n",
    "I had the following error when trying to run the code on Jupyter Notebook : \"The kernel appears to have died. It will restart automatically.\". According to [this](https://github.com/tensorflow/tensorflow/issues/9829#issuecomment-300783730), it must be run on a `.py` file.\n",
    "\n",
    "So please, you'll find the code for this question in the file `question43.py`.\n",
    "\n",
    "I got inspired by [this blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html): you need to download the [GloVe embeddings vectors](http://nlp.stanford.edu/data/glove.6B.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
